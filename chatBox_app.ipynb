{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump is the 45th President of the United States, serving from 2017 to 2021. He is a businessman, real estate developer, and television personality who was born on June 14, 1946, in Queens, New York.\n",
      "\n",
      "Before entering politics, Trump made a name for himself in the business world, building a real estate empire and creating a brand that became synonymous with luxury and success. He developed numerous high-end properties, including hotels, casinos, and golf courses, and wrote several bestselling books on business and entrepreneurship.\n",
      "\n",
      "Trump's entry into politics began in 2015, when he announced his candidacy for the Republican presidential nomination. Despite being a newcomer to politics, Trump's unconventional style and populist message resonated with many voters, and he won the Republican nomination in 2016.\n",
      "\n",
      "In the general election, Trump faced off against Democratic nominee Hillary Clinton, and his campaign was marked by controversy and polarization. Despite being a significant underdog, Trump won the election, carrying 304 electoral votes to Clinton's 227.\n",
      "\n",
      "As president, Trump's administration was marked by a number of significant events and policies, including:\n",
      "\n",
      "* The implementation of a travel ban targeting predominantly Muslim countries\n",
      "* The withdrawal of the United States from the Paris Climate Agreement\n",
      "* The passage of the Tax Cuts and Jobs Act, a major overhaul of the US tax code\n",
      "* A trade war with China, which led to tariffs on Chinese goods\n",
      "* The confirmation of two Supreme Court justices, Neil Gorsuch and Brett Kavanaugh\n",
      "\n",
      "Trump's presidency was also marked by controversy and scandal, including investigations into Russian interference in the 2016 election and allegations of obstruction of justice. In 2019, Trump became the third president in US history to be impeached by the House of Representatives, although he was acquitted by the Senate in 2020.\n",
      "\n",
      "Trump lost his bid for re-election in 2020 to Democratic nominee Joe Biden, and his presidency ended on January 20, 2021. Despite his departure from office, Trump remains a significant figure in American politics, and his legacy continues to be debated and contested.\n",
      "\n",
      "It's worth noting that Trump's presidency and legacy are highly polarizing, and opinions about him vary widely depending on one's political perspective. Some people view him as a successful businessman and a champion of conservative values, while others see him as a divisive figure who undermined democratic norms and institutions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    groq_api_key = \"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\",\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")\n",
    "result = llm.invoke(\"Who is Donald Trump?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341984/1007010242.py:18: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 995 unique questions in ChromaDB using LangChain!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_vector_db(file_path, persist_directory=\"./chroma_db\", collection_name=\"qa_collection\"):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if \"Context\" not in df.columns or \"Response\" not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain 'Context' and 'Response' columns.\")\n",
    "\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # Initialize ChromaDB collection\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,  \n",
    "        persist_directory=persist_directory,  \n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "    # Store questions with multiple responses\n",
    "    qa_dict = defaultdict(list)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        question = str(row[\"Context\"]).strip()\n",
    "        answer = str(row[\"Response\"]).strip()\n",
    "        qa_dict[question].append(answer)\n",
    "\n",
    "    # Convert questions and answers into LangChain Documents\n",
    "    docs = []\n",
    "    for question, answers in qa_dict.items():\n",
    "        answers_str = \"\\n\\n\".join(answers)  # Combine multiple answers\n",
    "        docs.append(Document(page_content=question, metadata={\"answers\": answers_str}))\n",
    "\n",
    "    # Add all documents to Chroma\n",
    "    vector_store.add_documents(docs)\n",
    "\n",
    "    print(f\"Stored {len(qa_dict)} unique questions in ChromaDB using LangChain!\")\n",
    "\n",
    "    return vector_store  # Return the vector store instance\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/home/wenjinf/NLP_Proj/train.csv\"\n",
    "vector_db = create_vector_db(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "  llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    groq_api_key = \"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\",\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")\n",
    "  return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Q: I need help knowing how to deal with stress. What can I do?\n",
      "   A: Something different works for each of us.There are the outward answers of self-pampering and making your home and work environment as pleasant as possible.A deeper level way to decrease stress is through exercise or alternative practices like yoga or tai chi.If the stress is more deeply rooted than temporarily feeling irritated for a few days, then give yourself some time to reflect and clarify what the meaning of the stress is to you.Self-understanding and appreciating your efforts to know yourself may decrease stress because you'll be more focused and attentive to who you are. Â This will influence you overall to make good decisions for yourself and these will naturally be ones which decrease stress as much as possible.\n",
      "\n",
      "2. Q: I need help dealing with stress. How can I handle it all and feel less stressed out?\n",
      "   A: Part of handling stress is making sure that your perception of the stress is accurate. Sometimes stress can seem more than it really is. One thing that I encourage my clients to do is to ask themselves, \"What is this stressor really about?\" Simplifying stress is a key to minimizing stress and leads to feeling less stressed out.\n",
      "\n",
      "3. Q: I am in a high stress position for a tech company. I am being overworked and underpaid for my contributions and it is not only giving me anxiety, but also demoralizing.\n",
      "   What can I do to manage my stress?\n",
      "   A: I think it's important to tease more of this situation out to figure out what is at the root of the stress. It is emotionally dangerous to be at a job for a lengthy duration in which you feel overworked and underpaid. You will not perform well as you mention, and thus your self-esteem will continually take a hit without really any effort. So, I don't know that simply coping with your stress would be advisable as a first step.You don't speak about a lot of what the office dynamics are like, which can be a big indicator for me of what can be done to help you feel better (because we exist as a part of a relationship with everything, including people at our job.) I would encourage you to speak up about your contributions to your boss. Often, \"overworked and underpaid\" also includes the \"my boss never notices me,\" and that can demoralizing. If we feel appreciated, that can go a long way. I've found that it is quite common for bosses to require some instruction for how to show each of their employees \"appreciation\" (and it goes deeper than \"thank you\" or taking you out to lunch - it's almost something felt as opposed to made explicit.)But sometimes appreciation isn't going to do the trick either. Because that overworked and underpaid actually has led you to feel \"burnout.\" You have zero interest in doing the job in the way it is designed, so some real changes need to be implemented.Â The bottom line? Try not to just \"suck it up\" and do all of the \"self care\" work on your own. If your company isn't helping you to take care of yourself (I'm talking to you, boss that handles employee pay and/or work conditions!) then you also have to question if this is a company worth working for. While I don't know exactly what you do, it sounds like you have confidence in your contributions! So take that confidence to a tech company that will support you (and there are tech companies out there!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def query_chromadb(user_query, top_k=3):\n",
    "    results = vector_db.similarity_search(user_query, k=top_k * 2)  # Get more than required\n",
    "\n",
    "    seen_questions = set()\n",
    "    unique_results = []\n",
    "\n",
    "    for result in results:\n",
    "        question = result.page_content\n",
    "        if question not in seen_questions:\n",
    "            seen_questions.add(question)\n",
    "            # Split the string into a list of answers\n",
    "            all_answers = result.metadata[\"answers\"].split(\"\\n\\n\")\n",
    "            response = random.choice(all_answers)  # Randomly select an answer\n",
    "            unique_results.append({\"question\": question, \"response\": response})\n",
    "        \n",
    "        if len(unique_results) == top_k:\n",
    "            break\n",
    "\n",
    "    return unique_results\n",
    "\n",
    "# Example Query\n",
    "user_question = \"How can I deal with stress?\"\n",
    "retrieved_answers = query_chromadb(user_question, top_k=3)\n",
    "\n",
    "# Display responses\n",
    "for idx, qa in enumerate(retrieved_answers, 1):\n",
    "    print(f\"{idx}. Q: {qa['question']}\\n   A: {qa['response']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def setup_qa_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "\n",
    "    def get_few_shot_examples(user_query, top_k=3):\n",
    "        \"\"\"Retrieve few-shot examples from ChromaDB.\"\"\"\n",
    "        examples = query_chromadb(user_query, top_k=top_k)\n",
    "        formatted_examples = \"\\n\\n\".join(\n",
    "            [f\"Example {idx}:\\nUser: {ex['question']}\\nChatbot: {ex['response']}\" for idx, ex in enumerate(examples, 1)]\n",
    "        )\n",
    "        return formatted_examples\n",
    "\n",
    "    \n",
    "    prompt_template = \"\"\"You are a compassionate mental health chatbot. Here are some past similar counseling conversation:\n",
    "    {context}\n",
    "\n",
    "    From the above examples, provide advice to the following user question:\n",
    "    User: {question}\n",
    "    Chatbot: \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])  # â 'question' not 'query'\n",
    "\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    def qa_chain_invoke(user_query):\n",
    "        \"\"\"Fetch few-shot examples and run RetrievalQA with the correct input format.\"\"\"\n",
    "        few_shot_examples = get_few_shot_examples(user_query)\n",
    "        return qa_chain.invoke({\"query\": user_query, \"context\": few_shot_examples})  # â Use 'question' instead of 'query'\n",
    "\n",
    "    return qa_chain_invoke  # Returns the function that dynamically injects few-shot examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so glad you reached out for support. Coping with anxiety can be challenging, but there are many strategies that can help. Based on what has worked for others, I'd like to suggest a few things that might be helpful for you.\n",
      "\n",
      "Firstly, it's essential to acknowledge that anxiety is a common experience, and it's not a sign of weakness. Many people have found it helpful to have a support system, whether it's a trusted friend, family member, or a mental health professional.\n",
      "\n",
      "Some people have found comfort in having an emotional support animal, which can provide a sense of calm and companionship. If you have a pet, spending time with them might help alleviate some of your anxiety.\n",
      "\n",
      "In terms of practical strategies, you might find it helpful to start small. For example, if you're feeling anxious about going to stores, you could start by going with a trusted friend or family member and gradually work up to going alone. You could also try practicing relaxation techniques, such as deep breathing, progressive muscle relaxation, or mindfulness meditation.\n",
      "\n",
      "It's also important to be kind to yourself and acknowledge that it's okay to take things at your own pace. Don't be too hard on yourself if you're not able to do something that feels overwhelming. Instead, focus on taking small steps towards your goals, and celebrate your successes along the way.\n",
      "\n",
      "Additionally, consider keeping a journal or talking to a trusted friend or family member about your feelings and experiences. Sometimes, just sharing your emotions with someone who cares about you can help you feel heard and understood.\n",
      "\n",
      "Lastly, if you're feeling overwhelmed and struggling to cope with your anxiety, consider reaching out to a mental health professional for support. They can help you develop a personalized plan to manage your anxiety and work through any underlying issues that may be contributing to your feelings.\n",
      "\n",
      "Remember, you're not alone in this. Many people have found ways to manage their anxiety and live fulfilling lives. With patience, support, and the right strategies, you can too.\n",
      "\n",
      "How do these suggestions resonate with you? Is there anything in particular that you'd like to try or talk more about?\n"
     ]
    }
   ],
   "source": [
    "# Setup the QA chain\n",
    "qa_chain = setup_qa_chain(vector_db, llm)\n",
    "\n",
    "# Example Query from the User\n",
    "user_question = \"How can I cope with anxiety?\"\n",
    "\n",
    "# Retrieve relevant responses with few-shot learning\n",
    "response = qa_chain(user_question)\n",
    "\n",
    "# Print the AI-generated response\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot...\n",
      "Chatbot: It's lovely to meet you. Is there something on your mind that you'd like to talk about, or are you just looking for someone to chat with? I'm here to listen and offer support if you need it. How's your day been so far?\n",
      "Chatbot: I'm so sorry to hear about the loss of your grandma. It's completely normal to feel empty and depressed after losing a loved one. Grieving is a process, and it's okay to take your time to work through your emotions.\n",
      "\n",
      "Firstly, please know that you're not alone in this feeling. It's essential to allow yourself to feel the emotions that come with grief, rather than trying to suppress or deny them. Acknowledge your feelings, and give yourself permission to grieve.\n",
      "\n",
      "Here are some suggestions that may help you cope with your emotions:\n",
      "\n",
      "1. **Talk to someone**: Reach out to a trusted friend, family member, or mental health professional who can listen to you without judgment. Sharing your feelings and memories of your grandma can be incredibly helpful.\n",
      "2. **Allow yourself to grieve**: Give yourself time to process your emotions. It's okay to cry, scream, or feel angry. Remember, grief is a unique and individual experience, and there's no set timeline for healing.\n",
      "3. **Create a memory book or ritual**: Consider creating a memory book, scrapbook, or a special ritual to honor your grandma's memory. This can help you feel more connected to her and process your emotions.\n",
      "4. **Take care of yourself**: Make sure you're getting enough rest, eating well, and engaging in activities that bring you comfort and relaxation. Exercise, meditation, or yoga can also help reduce stress and anxiety.\n",
      "5. **Seek support groups**: Joining a support group, either online or in-person, can connect you with others who have experienced a similar loss. Sharing your story and hearing others' experiences can be incredibly helpful.\n",
      "6. **Celebrate your grandma's life**: Think about the happy memories you shared with your grandma, and celebrate her life by doing things that remind you of her. This can help shift your focus from the pain of loss to the joy of the time you had together.\n",
      "\n",
      "Remember, healing is a journey, and it's okay to take it one step at a time. Be patient and kind to yourself as you navigate this difficult time.\n",
      "\n",
      "If you need someone to talk to, I'm here to listen and support you. How are you feeling right now, and what's been the most challenging part of your grief journey so far?\n",
      "Chatbot: Take care of yourself. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing Chatbot...\")\n",
    "\n",
    "    # Initialize LLM model (Ensure this function is defined elsewhere)\n",
    "    llm = initialize_llm()\n",
    "\n",
    "    db_path = \"./chroma_db\"\n",
    "\n",
    "    # Check if ChromaDB exists, if not, create it\n",
    "    if not os.path.exists(db_path):\n",
    "        vector_db = create_vector_db()  # Ensure this function is defined elsewhere\n",
    "    else:\n",
    "        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "\n",
    "    # Set up the QA chain and get the function that invokes responses\n",
    "    qa_chain_invoke = setup_qa_chain(vector_db, llm)  \n",
    "\n",
    "    # Interactive Chat Loop\n",
    "    while True:\n",
    "        query = input(\"\\nHuman: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Chatbot: Take care of yourself. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = qa_chain_invoke(query) \n",
    "        print(f\"Chatbot: {response['result']}\") \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://106a7a9b02c98ddff9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://106a7a9b02c98ddff9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Initialize the QA Chain\n",
    "qa_chain_invoke = setup_qa_chain(vector_db, llm)\n",
    "\n",
    "def chatbot_response(user_input, history=[]):\n",
    "    \"\"\"Handles user input and returns only the chatbot response as a string.\"\"\"\n",
    "    if not user_input.strip():\n",
    "        return \"Please provide a valid input\"  \n",
    "    \n",
    "    response = qa_chain_invoke(user_input)\n",
    "\n",
    "    \n",
    "    chatbot_reply = response if isinstance(response, str) else response.get(\"result\", \"Sorry, I couldn't understand.\")\n",
    "    \n",
    "    \n",
    "    history.append((user_input, chatbot_reply))\n",
    "\n",
    "    return chatbot_reply  \n",
    "\n",
    "\n",
    "with gr.Blocks(theme=\"shivi/calm_seafoam\") as app:\n",
    "    gr.Markdown(\"# ð§  Mental Health Chatbot ð¤\")\n",
    "    gr.Markdown(\"A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.\")\n",
    "\n",
    "    chatbot = gr.ChatInterface(fn=chatbot_response, title=\"Mental Health Chatbot\")\n",
    "\n",
    "    gr.Markdown(\"This chatbot provides general support. For urgent issues, seek help from licensed professionals.\")\n",
    "\n",
    "app.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: xgb_model.pkl\n",
      "Loaded label mapping from label_mapping.pkl\n",
      "6\n",
      "Predicted Category: Suicidal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenjinf/miniconda3/envs/py3.9/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:23:35] WARNING: /workspace/src/gbm/gbtree.cc:363: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/wenjinf/miniconda3/envs/py3.9/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:23:35] WARNING: /workspace/src/gbm/gbtree.cc:388: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/wenjinf/miniconda3/envs/py3.9/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:23:35] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/wenjinf/miniconda3/envs/py3.9/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:23:35] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def load_model(model_name):\n",
    "    model_filename = f\"{model_name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    with open(model_filename, \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Loaded model: {model_filename}\")\n",
    "    return model\n",
    "\n",
    "def load_label_mapping():\n",
    "    \"\"\"Load the label mapping from the pickle file.\"\"\"\n",
    "    with open(\"label_mapping.pkl\", \"rb\") as file:\n",
    "        label_mapping = pickle.load(file)\n",
    "    print(\"Loaded label mapping from label_mapping.pkl\")\n",
    "    return {v: k for k, v in label_mapping.items()} \n",
    "\n",
    "with open(\"tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "\n",
    "xgb_model = load_model(\"XGB\")\n",
    "label_mapping = load_label_mapping()\n",
    "\n",
    "# # Load required resources\n",
    "# nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    # Remove web URLs\n",
    "    input_text = re.sub(r\"https?://\\S+\", \"\", input_text)\n",
    "\n",
    "    # Remove markdown links of the format [text](url)\n",
    "    input_text = re.sub(r\"\\[.*?\\]\\(.*?\\)\", \"\", input_text)\n",
    "\n",
    "    input_text = re.sub(r'\\[.*?\\]', '', input_text)  # Remove text in square brackets\n",
    "\n",
    "    # Remove user mentions (handles starting with '@')\n",
    "    input_text = re.sub(r\"@\\w+\", \"\", input_text)\n",
    "\n",
    "    input_text = re.sub(r'\\n', '', input_text)\n",
    "\n",
    "    # Remove punctuation and special characters, keeping only words and spaces\n",
    "    input_text = re.sub(r\"[^\\w\\s]\", \"\", input_text)\n",
    "\n",
    "    return input_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(input_text, vectorizer):\n",
    "\n",
    "\n",
    "    char_count = len(input_text)\n",
    "    sentence_count = len(nltk.sent_tokenize(input_text))\n",
    "\n",
    "    cleaned_text = clean_text(input_text)\n",
    "\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    stemmed_tokens = ' '.join([porter.stem(word) for word in tokens])\n",
    "\n",
    "    input_tfidf = vectorizer.transform([stemmed_tokens])\n",
    "\n",
    "    input_numerical_features = np.array([[char_count, sentence_count]])\n",
    "\n",
    "    input_features = hstack([input_tfidf, input_numerical_features])\n",
    "\n",
    "    return input_features\n",
    "\n",
    "user_text = \"I feel like everyone dislikes me and excludes me, making me feel like a coward. Sometimes, it makes me want to kill myself.\"\n",
    "\n",
    "processed_input = preprocess_text(user_text, tfidf_vectorizer)\n",
    "\n",
    "# Make prediction using the loaded XGBoost model\n",
    "predicted_label = xgb_model.predict(processed_input)[0]\n",
    "print(predicted_label)\n",
    "predicted_class = label_mapping[predicted_label]\n",
    "\n",
    "print(f\"Predicted Category: {predicted_class}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://2e7ba64916f0eb1f92.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2e7ba64916f0eb1f92.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the QA Chain\n",
    "import gradio as gr\n",
    "qa_chain_invoke = setup_qa_chain(vector_db, llm)\n",
    "first_interaction = True  # Track first round\n",
    "user_mode = None  # Store user-selected mode\n",
    "\n",
    "def chatbot_response(user_input, history=[]):\n",
    "    \"\"\"Handles user input step by step, guiding users through selection, then answering.\"\"\"\n",
    "    global first_interaction, user_mode\n",
    "\n",
    "    if not user_input.strip():\n",
    "        return \"Please provide a valid input\"\n",
    "\n",
    "    if first_interaction:\n",
    "        # First greeting and mode selection\n",
    "        first_interaction = False  # Mark first round as completed\n",
    "        return (\n",
    "            \"Hello my friend, how can I help you today? \"\n",
    "            \"Typing the numeric number to proceed or 'exit' to end the conversation.\\n\"\n",
    "            \"1. Find examples of counseling sessions to get help on your questions.\\n\"\n",
    "            \"2. Get real-time advice online to solve your questions.\\n\"\n",
    "            \"3. Perdict likelihood of your health issue backened by trained ML model\"\n",
    "        )\n",
    "\n",
    "    # Handle user selecting mode (Step 2)\n",
    "    if user_mode is None:\n",
    "        if user_input.strip() == \"1\":\n",
    "            user_mode = 1\n",
    "            return \"Glad to help! What is your question?\"\n",
    "        elif user_input.strip() == \"2\":\n",
    "            user_mode = 2\n",
    "            return \"Glad to help! What is your question?\"\n",
    "        elif user_input.strip() == \"3\":\n",
    "            user_mode = 3\n",
    "            return \"Glad to help! What is your mental health state?\"\n",
    "        elif user_input.lower() == \"exit\":\n",
    "            return \"Chatbot: Take care of yourself. Goodbye!\"\n",
    "        else:\n",
    "            return \"Please select a valid option: 1, 2, 3 or exit\"\n",
    "\n",
    "    # Handle user's actual question (Step 3)\n",
    "    if user_mode == 1:\n",
    "        response_examples = query_chromadb(user_input, top_k=3)\n",
    "        response_text = \"\\n\\n\".join(\n",
    "            [f\"Example {idx}:\\nUser: {ex['question']}\\nChatbot: {ex['response']}\" for idx, ex in enumerate(response_examples, 1)]\n",
    "        )\n",
    "    elif user_mode == 2:\n",
    "        response_text = qa_chain_invoke(user_input)\n",
    "        response_text = response_text if isinstance(response_text, str) else response_text.get(\"result\", \"Sorry, I couldn't understand.\")\n",
    "    elif user_mode == 3:\n",
    "        processed_input = preprocess_text(user_input, tfidf_vectorizer)\n",
    "        # Make prediction using the loaded XGBoost model\n",
    "        predicted_label = xgb_model.predict(processed_input)[0]\n",
    "\n",
    "        predicted_class = label_mapping[predicted_label]\n",
    "\n",
    "        response_text = f\"You probably have {predicted_class} mental health issue!\"\n",
    "        \n",
    "\n",
    "    # Reset mode for next round\n",
    "    user_mode = None\n",
    "\n",
    "    #Prompt for next round**\n",
    "    next_prompt = (\n",
    "        \"\\n\\n\"\n",
    "        \"Hope my answer above helps! Do you have more questions?\\n\"\n",
    "        \"Typing the numeric number to proceed or 'exit' to end the conversation.\\n\"\n",
    "        \"1. Find examples of counseling sessions to get help on your questions.\\n\"\n",
    "        \"2. Get real-time advice online to solve your questions.\\n\"\n",
    "        \"3. Perdict likelihood of your health issue backened by trained ML model\"\n",
    "    )\n",
    "\n",
    "    return response_text + \"\\n\\n\" + next_prompt  # Response + next action prompt\n",
    "\n",
    "css=\"\"\".gradio-container .avatar-container {height: 40px width: 40px !important;} #duplicate-button {margin: auto; color: white; background: #f1a139; border-radius: 100vh;}\"\"\"\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=\"shivi/calm_seafoam\", css=css) as app:\n",
    "    gr.Markdown(\"# ð§  Mental Health Chatbot ð¤\")\n",
    "    gr.Markdown(\"A compassionate chatbot designed to assist with mental well-being.\")\n",
    "\n",
    "    chatbot = gr.ChatInterface(fn=chatbot_response, title=\"Mental Health Chatbot\")\n",
    "\n",
    "\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
